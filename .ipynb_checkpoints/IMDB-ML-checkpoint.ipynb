{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "88c6210d-12bb-4013-8c57-24edd9a04026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordcloud in c:\\users\\melikeavan\\anaconda3\\lib\\site-packages (1.9.4)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\melikeavan\\anaconda3\\lib\\site-packages (from wordcloud) (1.26.4)\n",
      "Requirement already satisfied: pillow in c:\\users\\melikeavan\\anaconda3\\lib\\site-packages (from wordcloud) (10.4.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\melikeavan\\anaconda3\\lib\\site-packages (from wordcloud) (3.10.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\melikeavan\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\melikeavan\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\melikeavan\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\melikeavan\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\melikeavan\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (24.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\melikeavan\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\melikeavan\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\melikeavan\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
      "Requirement already satisfied: spacy in c:\\users\\melikeavan\\anaconda3\\lib\\site-packages (3.8.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\melikeavan\\anaconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\melikeavan\\anaconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\melikeavan\\anaconda3\\lib\\site-packages (from spacy) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\melikeavan\\anaconda3\\lib\\site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\melikeavan\\anaconda3\\lib\\site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\melikeavan\\anaconda3\\lib\\site-packages (from spacy) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\melikeavan\\anaconda3\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\melikeavan\\anaconda3\\lib\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\melikeavan\\anaconda3\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\melikeavan\\anaconda3\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\melikeavan\\anaconda3\\lib\\site-packages (from spacy) (0.16.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\melikeavan\\anaconda3\\lib\\site-packages (from spacy) (4.66.5)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\melikeavan\\anaconda3\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\melikeavan\\anaconda3\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\melikeavan\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (2.10.6)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\melikeavan\\anaconda3\\lib\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\melikeavan\\anaconda3\\lib\\site-packages (from spacy) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\melikeavan\\anaconda3\\lib\\site-packages (from spacy) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\melikeavan\\anaconda3\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\melikeavan\\anaconda3\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\melikeavan\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\melikeavan\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\melikeavan\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\melikeavan\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\melikeavan\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\melikeavan\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\melikeavan\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in c:\\users\\melikeavan\\anaconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\melikeavan\\anaconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Collecting numpy>=1.19.0 (from spacy)\n",
      "  Using cached numpy-2.3.1-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\melikeavan\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\melikeavan\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\melikeavan\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\melikeavan\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.7.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\melikeavan\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\melikeavan\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\melikeavan\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\melikeavan\\anaconda3\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\melikeavan\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\melikeavan\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\melikeavan\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.0)\n",
      "Using cached numpy-2.3.1-cp312-cp312-win_amd64.whl (12.7 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "Successfully installed numpy-2.3.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\MelikeAVAN\\anaconda3\\Lib\\site-packages\\~umpy.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\MelikeAVAN\\anaconda3\\Lib\\site-packages\\~-mpy'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "albucore 0.0.23 requires opencv-python-headless>=4.9.0.80, which is not installed.\n",
      "albucore 0.0.23 requires simsimd>=5.9.2, which is not installed.\n",
      "albucore 0.0.23 requires stringzilla>=3.10.4, which is not installed.\n",
      "albumentations 2.0.2 requires opencv-python-headless>=4.9.0.80, which is not installed.\n",
      "contourpy 1.2.0 requires numpy<2.0,>=1.20, but you have numpy 2.3.1 which is incompatible.\n",
      "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.3.1 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.1 which is incompatible.\n",
      "scipy 1.13.1 requires numpy<2.3,>=1.22.4, but you have numpy 2.3.1 which is incompatible.\n",
      "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.3.1 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in c:\\users\\melikeavan\\anaconda3\\lib\\site-packages (0.19.0)\n",
      "Requirement already satisfied: nltk>=3.9 in c:\\users\\melikeavan\\anaconda3\\lib\\site-packages (from textblob) (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\melikeavan\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\melikeavan\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\melikeavan\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\melikeavan\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\melikeavan\\anaconda3\\lib\\site-packages (from click->nltk>=3.9->textblob) (0.4.6)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\melikeavan\\anaconda3\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\melikeavan\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\melikeavan\\anaconda3\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\melikeavan\\anaconda3\\lib\\site-packages (from scikit-learn) (2.3.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\melikeavan\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\melikeavan\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\melikeavan\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Collecting numpy>=1.19.5 (from scikit-learn)\n",
      "  Using cached numpy-2.2.6-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Using cached numpy-2.2.6-cp312-cp312-win_amd64.whl (12.6 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.3.1\n",
      "    Uninstalling numpy-2.3.1:\n",
      "      Successfully uninstalled numpy-2.3.1\n",
      "Successfully installed numpy-2.2.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "albucore 0.0.23 requires opencv-python-headless>=4.9.0.80, which is not installed.\n",
      "albucore 0.0.23 requires simsimd>=5.9.2, which is not installed.\n",
      "albucore 0.0.23 requires stringzilla>=3.10.4, which is not installed.\n",
      "albumentations 2.0.2 requires opencv-python-headless>=4.9.0.80, which is not installed.\n",
      "contourpy 1.2.0 requires numpy<2.0,>=1.20, but you have numpy 2.2.6 which is incompatible.\n",
      "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.2.6 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.6 which is incompatible.\n",
      "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.2.6 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install wordcloud\n",
    "!pip install spacy\n",
    "!pip install textblob\n",
    "!pip install beautifulsoup4\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "10e01a82-a6e7-4484-8fec-84722d7b3dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy<2.0\n",
      "  Using cached numpy-1.26.4-cp312-cp312-win_amd64.whl.metadata (61 kB)\n",
      "Using cached numpy-1.26.4-cp312-cp312-win_amd64.whl (15.5 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.6\n",
      "    Uninstalling numpy-2.2.6:\n",
      "      Successfully uninstalled numpy-2.2.6\n",
      "Successfully installed numpy-1.26.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "albucore 0.0.23 requires opencv-python-headless>=4.9.0.80, which is not installed.\n",
      "albucore 0.0.23 requires simsimd>=5.9.2, which is not installed.\n",
      "albucore 0.0.23 requires stringzilla>=3.10.4, which is not installed.\n",
      "albumentations 2.0.2 requires opencv-python-headless>=4.9.0.80, which is not installed.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install \"numpy<2.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "dfdac759-bed6-47d3-8189-7c55bd3ffde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "import spacy\n",
    "import re,string,unicodedata\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.stem import LancasterStemmer,WordNetLemmatizer\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "cbe7f67f-d67b-46ff-893e-d1fa1d18ae26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Probably my all-time favorite movie, a story o...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I sure would like to see a resurrection of a u...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This show was an amazing, fresh &amp; innovative i...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Encouraged by the positive comments about this...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>If you like original gut wrenching laughter yo...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "5  Probably my all-time favorite movie, a story o...  positive\n",
       "6  I sure would like to see a resurrection of a u...  positive\n",
       "7  This show was an amazing, fresh & innovative i...  negative\n",
       "8  Encouraged by the positive comments about this...  negative\n",
       "9  If you like original gut wrenching laughter yo...  positive"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_data= pd.read_csv(\"IMDB Dataset.csv\")\n",
    "print(imdb_data.shape)\n",
    "imdb_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b6e963f8-4c32-4394-8fd7-b7ab0b70804e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50000</td>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>49582</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Loved today's show!!! It was a variety and not...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>5</td>\n",
       "      <td>25000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   review sentiment\n",
       "count                                               50000     50000\n",
       "unique                                              49582         2\n",
       "top     Loved today's show!!! It was a variety and not...  positive\n",
       "freq                                                    5     25000"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Summary of the dataset\n",
    "imdb_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "08187685-6999-4858-af6c-82e12d601364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "positive    25000\n",
       "negative    25000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sentiment count\n",
    "imdb_data['sentiment'].value_counts()\n",
    "#We can see that the dataset is balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1d90f6d3-b8f5-4430-bb83-f34ad253c7b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000,) (40000,)\n",
      "(10000,) (10000,)\n"
     ]
    }
   ],
   "source": [
    "#Split the dataset  \n",
    "#Train dataset\n",
    "train_reviews=imdb_data.review[:40000]\n",
    "train_sentiments=imdb_data.sentiment[:40000]\n",
    "#Test dataset\n",
    "test_reviews=imdb_data.review[40000:]\n",
    "test_sentiments=imdb_data.sentiment[40000:]\n",
    "print(train_reviews.shape,train_sentiments.shape)\n",
    "print(test_reviews.shape,test_sentiments.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "13081566-98a3-4b5f-a924-34f106381eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c055bd9b-2d0e-4171-8d82-c221f8e55118",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\MelikeAVAN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Tokenization of text\n",
    "tokenizer=ToktokTokenizer()\n",
    "#Setting English stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stopword_list=nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9c4c60ac-d6c2-4110-ae32-e493ddb586da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing html strips and noise text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "78ce8d90-43c9-4bcc-97c0-7560311e4d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the html strips\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0b248e63-4dfc-47eb-b431-644ae0778060",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the square brackets\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub(r'\\[[^]]*\\]', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "839ba40e-ce4c-4ce9-8c08-2856d91b9ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the noisy text\n",
    "def denoise_text(text):\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3754f893-6e88-4692-9be8-36477946963f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MelikeAVAN\\AppData\\Local\\Temp\\ipykernel_8108\\1032338494.py:3: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n"
     ]
    }
   ],
   "source": [
    "# Application to clear data\n",
    "imdb_data['review'] = imdb_data['review'].apply(denoise_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "cfce3993-76e3-4385-a7b4-e147374aa736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "bc992c18-50f3-4624-92ed-a87c67910343",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define function for removing special characters\n",
    "def remove_special_characters(text, remove_digits=True):\n",
    "    pattern=r'[^a-zA-z0-9\\s]'\n",
    "    text=re.sub(pattern,'',text)\n",
    "    return text\n",
    "#Apply function on review column\n",
    "imdb_data['review']=imdb_data['review'].apply(remove_special_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e5331060-9710-418b-855e-47aec0685853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "745e0bbe-0a99-4c26-b86a-389f748a3b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming the text\n",
    "def simple_stemmer(text):\n",
    "    ps=nltk.porter.PorterStemmer()\n",
    "    text= ' '.join([ps.stem(word) for word in text.split()])\n",
    "    return text\n",
    "#Apply function on review column\n",
    "imdb_data['review']=imdb_data['review'].apply(simple_stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b7861157-1f7d-4fcc-bf51-f26cf85fd502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "db9faa63-be63-406c-8d3c-ea61ffda2e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"shan't\", 'did', \"you're\", 'why', \"isn't\", \"aren't\", 'their', 'o', \"he's\", 'or', 'shan', \"she'll\", 'there', 'once', 'his', \"he'd\", 'over', 'doesn', 'should', 'between', 'theirs', 'aren', 'all', \"it'd\", 'weren', \"mightn't\", 'mustn', \"i'd\", 'of', \"should've\", 'when', 'll', \"shouldn't\", 'above', 'ma', 'them', \"couldn't\", 'to', 'having', 'other', 'can', 'doing', 'm', 'the', 'does', 'an', 'below', 'not', 'they', 'how', 'didn', 'if', 'she', 'him', 'here', 'mightn', 'few', 'have', 'yours', 'shouldn', 'which', 'on', 'couldn', 'wouldn', 'we', 'am', 'as', \"they're\", 'haven', 'by', 'because', \"haven't\", 't', \"we're\", 'some', 'for', 'you', 'me', 'wasn', 'before', 'more', 'your', 'both', 'during', \"wouldn't\", 've', 'isn', 'who', \"i'm\", 'y', 'hers', 'while', 'against', 'until', 'whom', \"we've\", \"won't\", 'most', \"we'll\", 'down', 'were', 'into', 'my', 'own', 'being', 'that', 'after', 'then', \"they'd\", \"weren't\", \"hadn't\", \"you'd\", 'this', 'themselves', 'd', 'ourselves', \"he'll\", 'be', 'won', \"they'll\", 'i', 'do', 'out', 's', \"you'll\", 'these', \"it'll\", 'had', 'than', 'are', 'but', 'no', 'those', \"you've\", 'don', \"mustn't\", 'too', \"hasn't\", 'up', \"they've\", \"wasn't\", 'it', 'again', 'and', 'he', 'ain', 'now', 're', 'where', 'under', 'hasn', 'at', 'myself', 'nor', 'was', 'will', 'hadn', 'just', 'any', 'its', 'is', 'himself', 'with', \"doesn't\", 'ours', \"it's\", 'very', 'herself', \"i'll\", 'in', \"i've\", \"she'd\", \"she's\", 'a', 'off', 'been', 'what', 'further', \"needn't\", 'yourselves', 'has', 'her', 'yourself', 'each', \"don't\", 'itself', 'same', \"that'll\", \"we'd\", 'from', \"didn't\", 'our', 'only', 'so', 'about', 'such', 'through', 'needn'}\n"
     ]
    }
   ],
   "source": [
    "#Set stopwords to english\n",
    "stop=set(stopwords.words('english'))\n",
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ab15913b-fbc7-4d88-b988-5a44f0503bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing the stopwords\n",
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)    \n",
    "    return filtered_text\n",
    "#Apply function on review column\n",
    "imdb_data['review']=imdb_data['review'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "15456237-42c6-42c1-8d48-9a0eda473dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized train reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1c7e7c1c-4431-42e7-85c5-9d117874726d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'one review ha mention watch 1 oz episod youll hook right thi exactli happen meth first thing struck oz wa brutal unflinch scene violenc set right word go trust thi show faint heart timid thi show pull punch regard drug sex violenc hardcor classic use wordit call oz nicknam given oswald maximum secur state penitentari focus mainli emerald citi experiment section prison cell glass front face inward privaci high agenda em citi home manyaryan muslim gangsta latino christian italian irish moreso scuffl death stare dodgi deal shadi agreement never far awayi would say main appeal show due fact goe show wouldnt dare forget pretti pictur paint mainstream audienc forget charm forget romanceoz doesnt mess around first episod ever saw struck nasti wa surreal couldnt say wa readi watch develop tast oz got accustom high level graphic violenc violenc injustic crook guard wholl sold nickel inmat wholl kill order get away well manner middl class inmat turn prison bitch due lack street skill prison experi watch oz may becom comfort uncomfort viewingthat get touch darker side'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#normalized train reviews\n",
    "norm_train_reviews=imdb_data.review[:40000]\n",
    "norm_train_reviews[0]\n",
    "#convert dataframe to string\n",
    "#norm_train_string=norm_train_reviews.to_string()\n",
    "#Spelling correction using Textblob\n",
    "#norm_train_spelling=TextBlob(norm_train_string)\n",
    "#norm_train_spelling.correct()\n",
    "#Tokenization using Textblob\n",
    "#norm_train_words=norm_train_spelling.words\n",
    "#norm_train_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b9eba7db-af3d-4325-9c1e-ba04ab32080e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized test reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3510e2fa-9915-4ab8-903e-8dbbc101b5ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'read review watch thi piec cinemat garbag took least 2 page find somebodi els didnt think thi appallingli unfunni montag wasnt acm humour 70 inde ani era thi isnt least funni set sketch comedi ive ever seen itll till come along half skit alreadi done infinit better act monti python woodi allen wa say nice piec anim last 90 second highlight thi film would still get close sum mindless drivelridden thi wast 75 minut semin comedi onli world semin realli doe mean semen scatolog humour onli world scat actual fece precursor joke onli mean thi handbook comedi tit bum odd beaver niceif pubesc boy least one hand free havent found playboy exist give break becaus wa earli 70 way sketch comedi go back least ten year prior onli way could even forgiv thi film even made wa gunpoint retro hardli sketch clown subtli pervert children may cut edg circl could actual funni come realli quit sad kept go throughout entir 75 minut sheer belief may save genuin funni skit end gave film 1 becaus wa lower scoreand onli recommend insomniac coma patientsor perhap peopl suffer lockjawtheir jaw would final drop open disbelief'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Normalized test reviews\n",
    "norm_test_reviews=imdb_data.review[40000:]\n",
    "norm_test_reviews[45005]\n",
    "##convert dataframe to string\n",
    "#norm_test_string=norm_test_reviews.to_string()\n",
    "#spelling correction using Textblob\n",
    "#norm_test_spelling=TextBlob(norm_test_string)\n",
    "#print(norm_test_spelling.correct())\n",
    "#Tokenization using Textblob\n",
    "#norm_test_words=norm_test_spelling.words\n",
    "#norm_test_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "addbc4a9-3639-4baf-9393-490c61f3efeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bags of words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "0d1f5764-2ab0-4619-a94b-0f5772c12d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW_cv_train: (40000, 6983231)\n",
      "BOW_cv_test: (10000, 6983231)\n"
     ]
    }
   ],
   "source": [
    "#Count vectorizer for bag of words\n",
    "cv=CountVectorizer(min_df=0.0,max_df=1.0,binary=False,ngram_range=(1,3))\n",
    "#transformed train reviews\n",
    "cv_train_reviews=cv.fit_transform(norm_train_reviews)\n",
    "#transformed test reviews\n",
    "cv_test_reviews=cv.transform(norm_test_reviews)\n",
    "\n",
    "print('BOW_cv_train:',cv_train_reviews.shape)\n",
    "print('BOW_cv_test:',cv_test_reviews.shape)\n",
    "#vocab=cv.get_feature_names()-toget feature names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "4acf3112-e0c7-4f27-8f38-a13aabd1d1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Term Frequency-Inverse Document Frequency model (TFIDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6b132a50-fb01-4837-96bc-b6102161f556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tfidf_train: (40000, 6983231)\n",
      "Tfidf_test: (10000, 6983231)\n"
     ]
    }
   ],
   "source": [
    "#it is used to convert text documents to matrix of tfidf features\n",
    "#Tfidf vectorizer\n",
    "tv=TfidfVectorizer(min_df=0.0,max_df=1.0,use_idf=True,ngram_range=(1,3))\n",
    "#transformed train reviews\n",
    "tv_train_reviews=tv.fit_transform(norm_train_reviews)\n",
    "#transformed test reviews\n",
    "tv_test_reviews=tv.transform(norm_test_reviews)\n",
    "print('Tfidf_train:',tv_train_reviews.shape)\n",
    "print('Tfidf_test:',tv_test_reviews.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "74bb239f-28a6-4529-8d91-1a1ee988fb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labeling the sentiment text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c1a96d91-67e9-4ce7-836d-66f3184a076d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 1)\n"
     ]
    }
   ],
   "source": [
    "#labeling the sentient data\n",
    "lb=LabelBinarizer()\n",
    "#transformed sentiment data\n",
    "sentiment_data=lb.fit_transform(imdb_data['sentiment'])\n",
    "print(sentiment_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b9270793-79cd-48fd-84bf-b44971536318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the sentiment tdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "acb9cfc9-c19f-4070-9aa4-7ec3f6775bf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " ...\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " ...\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "#Spliting the sentiment data\n",
    "train_sentiments=sentiment_data[:40000]\n",
    "test_sentiments=sentiment_data[40000:]\n",
    "print(train_sentiments)\n",
    "print(test_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "8cb3928a-7556-43d4-903d-93f09bdeae3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelling the dataset\n",
    "\n",
    "# logistic regression model for both bag of words and tfidf features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "42bdf647-3c39-4865-8f58-884ab13a28ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MelikeAVAN\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1, max_iter=500, random_state=42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MelikeAVAN\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1, max_iter=500, random_state=42)\n"
     ]
    }
   ],
   "source": [
    "#training the model\n",
    "lr=LogisticRegression(penalty='l2',max_iter=500,C=1,random_state=42)\n",
    "#Fitting the model for Bag of words\n",
    "lr_bow=lr.fit(cv_train_reviews,train_sentiments)\n",
    "print(lr_bow)\n",
    "#Fitting the model for tfidf features\n",
    "lr_tfidf=lr.fit(tv_train_reviews,train_sentiments)\n",
    "print(lr_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "783fdc2b-1b44-4888-9dad-d870da0089ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n",
      "[0 0 0 ... 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "#Predicting the model for bag of words\n",
    "lr_bow_predict=lr.predict(cv_test_reviews)\n",
    "print(lr_bow_predict)\n",
    "##Predicting the model for tfidf features\n",
    "lr_tfidf_predict=lr.predict(tv_test_reviews)\n",
    "print(lr_tfidf_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "043c2b27-70c7-4a87-92c8-9744de5ec670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr_bow_score : 0.8397\n",
      "lr_tfidf_score : 0.8873\n"
     ]
    }
   ],
   "source": [
    "#Accuracy score for bag of words\n",
    "lr_bow_score=accuracy_score(test_sentiments,lr_bow_predict)\n",
    "print(\"lr_bow_score :\",lr_bow_score)\n",
    "#Accuracy score for tfidf features\n",
    "lr_tfidf_score=accuracy_score(test_sentiments,lr_tfidf_predict)\n",
    "print(\"lr_tfidf_score :\",lr_tfidf_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b0472dba-a599-4d7e-b311-d7a241ab343c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.80      0.91      0.85      4993\n",
      "    Negative       0.90      0.77      0.83      5007\n",
      "\n",
      "    accuracy                           0.84     10000\n",
      "   macro avg       0.85      0.84      0.84     10000\n",
      "weighted avg       0.85      0.84      0.84     10000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.89      0.89      0.89      4993\n",
      "    Negative       0.89      0.89      0.89      5007\n",
      "\n",
      "    accuracy                           0.89     10000\n",
      "   macro avg       0.89      0.89      0.89     10000\n",
      "weighted avg       0.89      0.89      0.89     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Classification report for bag of words \n",
    "lr_bow_report=classification_report(test_sentiments,lr_bow_predict,target_names=['Positive','Negative'])\n",
    "print(lr_bow_report)\n",
    "\n",
    "#Classification report for tfidf features\n",
    "lr_tfidf_report=classification_report(test_sentiments,lr_tfidf_predict,target_names=['Positive','Negative'])\n",
    "print(lr_tfidf_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d6716067-17a4-4799-a7e9-de74e6fe651b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3831 1176]\n",
      " [ 427 4566]]\n",
      "[[4447  560]\n",
      " [ 567 4426]]\n"
     ]
    }
   ],
   "source": [
    "#confusion matrix for bag of words\n",
    "cm_bow=confusion_matrix(test_sentiments,lr_bow_predict,labels=[1,0])\n",
    "print(cm_bow)\n",
    "#confusion matrix for tfidf features\n",
    "cm_tfidf=confusion_matrix(test_sentiments,lr_tfidf_predict,labels=[1,0])\n",
    "print(cm_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "491f7015-382c-4985-8dc9-8b8e43b222cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stochastic gradient descent or Linear support vector machines for bag of words and tfidf features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "d16ab73a-d2b6-43e8-a515-0013b0773d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MelikeAVAN\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier(max_iter=500, random_state=42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MelikeAVAN\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier(max_iter=500, random_state=42)\n"
     ]
    }
   ],
   "source": [
    "#training the linear svm\n",
    "svm=SGDClassifier(loss='hinge',max_iter=500,random_state=42)\n",
    "#fitting the svm for bag of words\n",
    "svm_bow=svm.fit(cv_train_reviews,train_sentiments)\n",
    "print(svm_bow)\n",
    "#fitting the svm for tfidf features\n",
    "svm_tfidf=svm.fit(tv_train_reviews,train_sentiments)\n",
    "print(svm_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a31a8b78-e9fb-4d0f-beed-9e129d5fe92f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n",
      "[0 0 0 ... 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "#Predicting the model for bag of words\n",
    "svm_bow_predict=svm.predict(cv_test_reviews)\n",
    "print(svm_bow_predict)\n",
    "#Predicting the model for tfidf features\n",
    "svm_tfidf_predict=svm.predict(tv_test_reviews)\n",
    "print(svm_tfidf_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "1ee15ba9-63ee-4a38-b6b6-fb46f6d103e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm_bow_score : 0.8433\n",
      "svm_tfidf_score : 0.8869\n"
     ]
    }
   ],
   "source": [
    "#Accuracy score for bag of words\n",
    "svm_bow_score=accuracy_score(test_sentiments,svm_bow_predict)\n",
    "print(\"svm_bow_score :\",svm_bow_score)\n",
    "#Accuracy score for tfidf features\n",
    "svm_tfidf_score=accuracy_score(test_sentiments,svm_tfidf_predict)\n",
    "print(\"svm_tfidf_score :\",svm_tfidf_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "26146903-305c-4b40-a6a1-4ba25d195e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.80      0.91      0.85      4993\n",
      "    Negative       0.90      0.77      0.83      5007\n",
      "\n",
      "    accuracy                           0.84     10000\n",
      "   macro avg       0.85      0.84      0.84     10000\n",
      "weighted avg       0.85      0.84      0.84     10000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.90      0.88      0.89      4993\n",
      "    Negative       0.88      0.90      0.89      5007\n",
      "\n",
      "    accuracy                           0.89     10000\n",
      "   macro avg       0.89      0.89      0.89     10000\n",
      "weighted avg       0.89      0.89      0.89     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Classification report for bag of words \n",
    "svm_bow_report=classification_report(test_sentiments,svm_bow_predict,target_names=['Positive','Negative'])\n",
    "print(svm_bow_report)\n",
    "#Classification report for tfidf features\n",
    "svm_tfidf_report=classification_report(test_sentiments,svm_tfidf_predict,target_names=['Positive','Negative'])\n",
    "print(svm_tfidf_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b804b672-3c12-4999-a0b8-8726ae0f3882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3878 1129]\n",
      " [ 438 4555]]\n",
      "[[4497  510]\n",
      " [ 621 4372]]\n"
     ]
    }
   ],
   "source": [
    "#confusion matrix for bag of words\n",
    "cm_bow=confusion_matrix(test_sentiments,svm_bow_predict,labels=[1,0])\n",
    "print(cm_bow)\n",
    "#confusion matrix for tfidf features\n",
    "cm_tfidf=confusion_matrix(test_sentiments,svm_tfidf_predict,labels=[1,0])\n",
    "print(cm_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "eeae6229-28bc-4fec-be4f-59e720a8f4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multinomial Naive Bayes for bag of words and tfidf features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4c92198b-64d2-4302-a732-5fcd4b7bdd4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MelikeAVAN\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MelikeAVAN\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB()\n"
     ]
    }
   ],
   "source": [
    "#training the model\n",
    "mnb=MultinomialNB()\n",
    "#fitting the svm for bag of words\n",
    "mnb_bow=mnb.fit(cv_train_reviews,train_sentiments)\n",
    "print(mnb_bow)\n",
    "#fitting the svm for tfidf features\n",
    "mnb_tfidf=mnb.fit(tv_train_reviews,train_sentiments)\n",
    "print(mnb_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "a2afcc26-ff27-4101-98f2-3c75e26b88aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "#Predicting the model for bag of words\n",
    "mnb_bow_predict=mnb.predict(cv_test_reviews)\n",
    "print(mnb_bow_predict)\n",
    "#Predicting the model for tfidf features\n",
    "mnb_tfidf_predict=mnb.predict(tv_test_reviews)\n",
    "print(mnb_tfidf_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "0bcabebc-d971-46b7-9b69-c172337e9869",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnb_bow_score : 0.8783\n",
      "mnb_tfidf_score : 0.8892\n"
     ]
    }
   ],
   "source": [
    "#Accuracy score for bag of words\n",
    "mnb_bow_score=accuracy_score(test_sentiments,mnb_bow_predict)\n",
    "print(\"mnb_bow_score :\",mnb_bow_score)\n",
    "#Accuracy score for tfidf features\n",
    "mnb_tfidf_score=accuracy_score(test_sentiments,mnb_tfidf_predict)\n",
    "print(\"mnb_tfidf_score :\",mnb_tfidf_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ba1caefa-df92-4e29-bbcf-da363cf757ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.85      0.91      0.88      4993\n",
      "    Negative       0.91      0.84      0.87      5007\n",
      "\n",
      "    accuracy                           0.88     10000\n",
      "   macro avg       0.88      0.88      0.88     10000\n",
      "weighted avg       0.88      0.88      0.88     10000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.88      0.90      0.89      4993\n",
      "    Negative       0.90      0.88      0.89      5007\n",
      "\n",
      "    accuracy                           0.89     10000\n",
      "   macro avg       0.89      0.89      0.89     10000\n",
      "weighted avg       0.89      0.89      0.89     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Classification report for bag of words \n",
    "mnb_bow_report=classification_report(test_sentiments,mnb_bow_predict,target_names=['Positive','Negative'])\n",
    "print(mnb_bow_report)\n",
    "#Classification report for tfidf features\n",
    "mnb_tfidf_report=classification_report(test_sentiments,mnb_tfidf_predict,target_names=['Positive','Negative'])\n",
    "print(mnb_tfidf_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "61a9a9b4-63ad-4a2c-acde-ff4224176db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4218  789]\n",
      " [ 428 4565]]\n",
      "[[4391  616]\n",
      " [ 492 4501]]\n"
     ]
    }
   ],
   "source": [
    "#confusion matrix for bag of words\n",
    "cm_bow=confusion_matrix(test_sentiments,mnb_bow_predict,labels=[1,0])\n",
    "print(cm_bow)\n",
    "#confusion matrix for tfidf features\n",
    "cm_tfidf=confusion_matrix(test_sentiments,mnb_tfidf_predict,labels=[1,0])\n",
    "print(cm_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "aff223e9-e4b5-4358-a41f-831138f832d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "acb7f41b-9436-4689-beb0-67042435cc47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MelikeAVAN\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1389: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(max_depth=10, min_samples_split=10, n_estimators=50,\n",
      "                       n_jobs=-1, random_state=42)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MelikeAVAN\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1389: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(max_depth=10, min_samples_split=10, n_estimators=50,\n",
      "                       n_jobs=-1, random_state=42)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Modeli tanmla\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=50,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Bag of Words iin eit\n",
    "rf_bow = rf.fit(cv_train_reviews, train_sentiments)\n",
    "print(rf_bow)\n",
    "\n",
    "# TF-IDF iin eit\n",
    "rf_tfidf = rf.fit(tv_train_reviews, train_sentiments)\n",
    "print(rf_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "05000208-d236-4b20-bb51-68a24a61e687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 ... 1 0 1]\n",
      "[0 0 1 ... 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Tahminler\n",
    "rf_bow_predict = rf.predict(cv_test_reviews)\n",
    "print(rf_bow_predict)\n",
    "\n",
    "rf_tfidf_predict = rf.predict(tv_test_reviews)\n",
    "print(rf_tfidf_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "078f0f68-5498-41d4-b542-2df3a1b68f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rf_bow_score: 0.7477\n",
      "rf_tfidf_score: 0.7526\n"
     ]
    }
   ],
   "source": [
    "# Doruluk skorlar\n",
    "rf_bow_score = accuracy_score(test_sentiments, rf_bow_predict)\n",
    "print(\"rf_bow_score:\", rf_bow_score)\n",
    "\n",
    "rf_tfidf_score = accuracy_score(test_sentiments, rf_tfidf_predict)\n",
    "print(\"rf_tfidf_score:\", rf_tfidf_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "8f407393-99ef-4152-927d-f6d4af060833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.76      0.73      0.74      4993\n",
      "    Negative       0.74      0.77      0.75      5007\n",
      "\n",
      "    accuracy                           0.75     10000\n",
      "   macro avg       0.75      0.75      0.75     10000\n",
      "weighted avg       0.75      0.75      0.75     10000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.76      0.74      0.75      4993\n",
      "    Negative       0.74      0.77      0.76      5007\n",
      "\n",
      "    accuracy                           0.75     10000\n",
      "   macro avg       0.75      0.75      0.75     10000\n",
      "weighted avg       0.75      0.75      0.75     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification reports\n",
    "rf_bow_report = classification_report(test_sentiments, rf_bow_predict, target_names=['Positive', 'Negative'])\n",
    "print(rf_bow_report)\n",
    "\n",
    "rf_tfidf_report = classification_report(test_sentiments, rf_tfidf_predict, target_names=['Positive', 'Negative'])\n",
    "print(rf_tfidf_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "c0f02adb-9a12-4050-8d65-64ebf1c958b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3831 1176]\n",
      " [1347 3646]]\n",
      "[[3854 1153]\n",
      " [1321 3672]]\n"
     ]
    }
   ],
   "source": [
    "# Confusion matrices\n",
    "cm_bow_rf = confusion_matrix(test_sentiments, rf_bow_predict, labels=[1, 0])\n",
    "print(cm_bow_rf)\n",
    "\n",
    "cm_tfidf_rf = confusion_matrix(test_sentiments, rf_tfidf_predict, labels=[1, 0])\n",
    "print(cm_tfidf_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "b819b4f3-cc89-4477-84bd-24a6af8370eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "1d5a6687-818d-4f2e-945c-8c05318de80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(max_depth=20, random_state=42)\n",
      "DecisionTreeClassifier(max_depth=20, random_state=42)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Modeli tanmla\n",
    "dt = DecisionTreeClassifier(\n",
    "    max_depth=20,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    random_state=42\n",
    ")\n",
    "# Bag of Words iin eit\n",
    "dt_bow = dt.fit(cv_train_reviews, train_sentiments)\n",
    "print(dt_bow)\n",
    "\n",
    "# TF-IDF iin eit\n",
    "dt_tfidf = dt.fit(tv_train_reviews, train_sentiments)\n",
    "print(dt_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "6d85963c-6c9e-41a3-bde1-e53173f92026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 ... 1 1 0]\n",
      "[0 0 1 ... 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "# Tahminler\n",
    "dt_bow_predict = dt.predict(cv_test_reviews)\n",
    "print(dt_bow_predict)\n",
    "\n",
    "dt_tfidf_predict = dt.predict(tv_test_reviews)\n",
    "print(dt_tfidf_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "32bf4164-965a-434e-ad8b-f0384a8489c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dt_bow_score: 0.6764\n",
      "dt_tfidf_score: 0.7197\n"
     ]
    }
   ],
   "source": [
    "# Doruluk skorlar\n",
    "dt_bow_score = accuracy_score(test_sentiments, dt_bow_predict)\n",
    "print(\"dt_bow_score:\", dt_bow_score)\n",
    "\n",
    "dt_tfidf_score = accuracy_score(test_sentiments, dt_tfidf_predict)\n",
    "print(\"dt_tfidf_score:\", dt_tfidf_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "199ec64a-cd8c-4dbf-bc56-808bca458d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.73      0.56      0.64      4993\n",
      "    Negative       0.64      0.79      0.71      5007\n",
      "\n",
      "    accuracy                           0.68     10000\n",
      "   macro avg       0.69      0.68      0.67     10000\n",
      "weighted avg       0.69      0.68      0.67     10000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.76      0.64      0.70      4993\n",
      "    Negative       0.69      0.80      0.74      5007\n",
      "\n",
      "    accuracy                           0.72     10000\n",
      "   macro avg       0.72      0.72      0.72     10000\n",
      "weighted avg       0.72      0.72      0.72     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification reports\n",
    "dt_bow_report = classification_report(test_sentiments, dt_bow_predict, target_names=['Positive', 'Negative'])\n",
    "print(dt_bow_report)\n",
    "\n",
    "dt_tfidf_report = classification_report(test_sentiments, dt_tfidf_predict, target_names=['Positive', 'Negative'])\n",
    "print(dt_tfidf_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "072ab666-ffb9-4d8c-92e4-23fb7e603fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3949 1058]\n",
      " [2178 2815]]\n",
      "[[3982 1025]\n",
      " [1778 3215]]\n"
     ]
    }
   ],
   "source": [
    "# Confusion matrices\n",
    "cm_bow_dt = confusion_matrix(test_sentiments, dt_bow_predict, labels=[1, 0])\n",
    "print(cm_bow_dt)\n",
    "\n",
    "cm_tfidf_dt = confusion_matrix(test_sentiments, dt_tfidf_predict, labels=[1, 0])\n",
    "print(cm_tfidf_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "e5a414af-6444-44a7-be50-ad66d0b82517",
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "553c0baf-6845-40bf-a94a-e7c1e83c9d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier()\n",
      "KNeighborsClassifier()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MelikeAVAN\\anaconda3\\Lib\\site-packages\\sklearn\\neighbors\\_classification.py:239: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n",
      "C:\\Users\\MelikeAVAN\\anaconda3\\Lib\\site-packages\\sklearn\\neighbors\\_classification.py:239: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return self._fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Modeli tanmla\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Bag of Words iin eit\n",
    "knn_bow = knn.fit(cv_train_reviews, train_sentiments)\n",
    "print(knn_bow)\n",
    "\n",
    "# TF-IDF iin eit\n",
    "knn_tfidf = knn.fit(tv_train_reviews, train_sentiments)\n",
    "print(knn_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "804cbaca-b38f-49ba-8a39-50691ed86c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 1 0]\n",
      "[0 0 0 ... 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "# Tahminler\n",
    "knn_bow_predict = knn.predict(cv_test_reviews)\n",
    "print(knn_bow_predict)\n",
    "\n",
    "knn_tfidf_predict = knn.predict(tv_test_reviews)\n",
    "print(knn_tfidf_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "c1f93d12-163f-4b86-b029-7f179e1f0299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn_bow_score: 0.7244\n",
      "knn_tfidf_score: 0.7944\n"
     ]
    }
   ],
   "source": [
    "# Doruluk skorlar\n",
    "knn_bow_score = accuracy_score(test_sentiments, knn_bow_predict)\n",
    "print(\"knn_bow_score:\", knn_bow_score)\n",
    "\n",
    "knn_tfidf_score = accuracy_score(test_sentiments, knn_tfidf_predict)\n",
    "print(\"knn_tfidf_score:\", knn_tfidf_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "83ecc54c-3217-4cc4-b15d-3ddbae8667b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.69      0.80      0.74      4993\n",
      "    Negative       0.77      0.65      0.70      5007\n",
      "\n",
      "    accuracy                           0.72     10000\n",
      "   macro avg       0.73      0.72      0.72     10000\n",
      "weighted avg       0.73      0.72      0.72     10000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.84      0.73      0.78      4993\n",
      "    Negative       0.76      0.86      0.81      5007\n",
      "\n",
      "    accuracy                           0.79     10000\n",
      "   macro avg       0.80      0.79      0.79     10000\n",
      "weighted avg       0.80      0.79      0.79     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification reports\n",
    "knn_bow_report = classification_report(test_sentiments, knn_bow_predict, target_names=['Positive', 'Negative'])\n",
    "print(knn_bow_report)\n",
    "\n",
    "knn_tfidf_report = classification_report(test_sentiments, knn_tfidf_predict, target_names=['Positive', 'Negative'])\n",
    "print(knn_tfidf_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "cb0925bb-c005-44d5-bff9-482073986d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3240 1767]\n",
      " [ 989 4004]]\n",
      "[[4290  717]\n",
      " [1339 3654]]\n"
     ]
    }
   ],
   "source": [
    "# Confusion matrices\n",
    "cm_bow_knn = confusion_matrix(test_sentiments, knn_bow_predict, labels=[1, 0])\n",
    "print(cm_bow_knn)\n",
    "\n",
    "cm_tfidf_knn = confusion_matrix(test_sentiments, knn_tfidf_predict, labels=[1, 0])\n",
    "print(cm_tfidf_knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "47831bc0-fd8a-426a-b116-088fd7f5f44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "17b5f94d-6d5a-4060-b565-14812d49a672",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MelikeAVAN\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\MelikeAVAN\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1249: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\MelikeAVAN\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1408: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC(random_state=42)\n",
      "LinearSVC(random_state=42)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Linear Kernel ile SVM modelini tanmla\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svm_linear = LinearSVC(max_iter=1000, random_state=42)\n",
    "\n",
    "# Bag of Words iin eit\n",
    "svm_linear_bow = svm_linear.fit(cv_train_reviews, train_sentiments)\n",
    "print(svm_linear_bow)\n",
    "\n",
    "# TF-IDF iin eit\n",
    "svm_linear_tfidf = svm_linear.fit(tv_train_reviews, train_sentiments)\n",
    "print(svm_linear_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "f1956e72-1c15-46bd-81eb-7cb6dc80bd93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 1 0 0]\n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Tahminler\n",
    "svm_linear_bow_predict = svm_linear.predict(cv_test_reviews)\n",
    "print(svm_linear_bow_predict)\n",
    "\n",
    "svm_linear_tfidf_predict = svm_linear.predict(tv_test_reviews)\n",
    "print(svm_linear_tfidf_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "8a9fed12-50d7-428f-af81-df801386c000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm_linear_bow_score: 0.8838\n",
      "svm_linear_tfidf_score: 0.9044\n"
     ]
    }
   ],
   "source": [
    "# Doruluk skorlar\n",
    "svm_linear_bow_score = accuracy_score(test_sentiments, svm_linear_bow_predict)\n",
    "print(\"svm_linear_bow_score:\", svm_linear_bow_score)\n",
    "\n",
    "svm_linear_tfidf_score = accuracy_score(test_sentiments, svm_linear_tfidf_predict)\n",
    "print(\"svm_linear_tfidf_score:\", svm_linear_tfidf_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "9e245f82-f45b-4405-b3f2-48061c8287f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.88      0.89      0.88      4993\n",
      "    Negative       0.89      0.87      0.88      5007\n",
      "\n",
      "    accuracy                           0.88     10000\n",
      "   macro avg       0.88      0.88      0.88     10000\n",
      "weighted avg       0.88      0.88      0.88     10000\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.91      0.90      0.90      4993\n",
      "    Negative       0.90      0.91      0.91      5007\n",
      "\n",
      "    accuracy                           0.90     10000\n",
      "   macro avg       0.90      0.90      0.90     10000\n",
      "weighted avg       0.90      0.90      0.90     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification reports\n",
    "svm_linear_bow_report = classification_report(test_sentiments, svm_linear_bow_predict, target_names=['Positive', 'Negative'])\n",
    "print(svm_linear_bow_report)\n",
    "\n",
    "svm_linear_tfidf_report = classification_report(test_sentiments, svm_linear_tfidf_predict, target_names=['Positive', 'Negative'])\n",
    "print(svm_linear_tfidf_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "17e67e77-1b24-4cd0-acab-087a712818f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4374  633]\n",
      " [ 529 4464]]\n",
      "[[4565  442]\n",
      " [ 514 4479]]\n"
     ]
    }
   ],
   "source": [
    "# Confusion matrices\n",
    "cm_bow_linear = confusion_matrix(test_sentiments, svm_linear_bow_predict, labels=[1, 0])\n",
    "print(cm_bow_linear)\n",
    "\n",
    "cm_tfidf_linear = confusion_matrix(test_sentiments, svm_linear_tfidf_predict, labels=[1, 0])\n",
    "print(cm_tfidf_linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "d1e3a2a2-54bb-42a2-aa6f-2122e92910cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM RBF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "4bf72da5-f875-40ad-900f-98f74d99d290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom sklearn.svm import SVC\\n\\n# RBF Kernel ile SVM modelini tanmla\\nsvm_rbf = SVC(kernel='rbf', random_state=42)\\n\\n# Bag of Words iin eit\\nsvm_rbf_bow = svm_rbf.fit(cv_train_reviews, train_sentiments)\\nprint(svm_rbf_bow)\\n\\n# TF-IDF iin eit\\nsvm_rbf_tfidf = svm_rbf.fit(tv_train_reviews, train_sentiments)\\nprint(svm_rbf_tfidf)\\n\""
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# RBF Kernel ile SVM modelini tanmla\n",
    "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
    "\n",
    "# Bag of Words iin eit\n",
    "svm_rbf_bow = svm_rbf.fit(cv_train_reviews, train_sentiments)\n",
    "print(svm_rbf_bow)\n",
    "\n",
    "# TF-IDF iin eit\n",
    "svm_rbf_tfidf = svm_rbf.fit(tv_train_reviews, train_sentiments)\n",
    "print(svm_rbf_tfidf)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "429805e7-0785-4f0f-93f7-aae1630d087a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Tahminler\\nsvm_rbf_bow_predict = svm_rbf.predict(cv_test_reviews)\\nprint(svm_rbf_bow_predict)\\n\\nsvm_rbf_tfidf_predict = svm_rbf.predict(tv_test_reviews)\\nprint(svm_rbf_tfidf_predict)\\n'"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Tahminler\n",
    "svm_rbf_bow_predict = svm_rbf.predict(cv_test_reviews)\n",
    "print(svm_rbf_bow_predict)\n",
    "\n",
    "svm_rbf_tfidf_predict = svm_rbf.predict(tv_test_reviews)\n",
    "print(svm_rbf_tfidf_predict)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "8b4cb37b-dde9-4dce-b5cc-75c6bdc6613f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Doruluk skorlar\\nsvm_rbf_bow_score = accuracy_score(test_sentiments, svm_rbf_bow_predict)\\nprint(\"svm_rbf_bow_score:\", svm_rbf_bow_score)\\n\\nsvm_rbf_tfidf_score = accuracy_score(test_sentiments, svm_rbf_tfidf_predict)\\nprint(\"svm_rbf_tfidf_score:\", svm_rbf_tfidf_score)\\n'"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Doruluk skorlar\n",
    "svm_rbf_bow_score = accuracy_score(test_sentiments, svm_rbf_bow_predict)\n",
    "print(\"svm_rbf_bow_score:\", svm_rbf_bow_score)\n",
    "\n",
    "svm_rbf_tfidf_score = accuracy_score(test_sentiments, svm_rbf_tfidf_predict)\n",
    "print(\"svm_rbf_tfidf_score:\", svm_rbf_tfidf_score)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "f2ebc364-df1e-43e0-96b6-ca4319da9320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Classification reports\\nsvm_rbf_bow_report = classification_report(test_sentiments, svm_rbf_bow_predict, target_names=['Positive', 'Negative'])\\nprint(svm_rbf_bow_report)\\n\\nsvm_rbf_tfidf_report = classification_report(test_sentiments, svm_rbf_tfidf_predict, target_names=['Positive', 'Negative'])\\nprint(svm_rbf_tfidf_report)\\n\""
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Classification reports\n",
    "svm_rbf_bow_report = classification_report(test_sentiments, svm_rbf_bow_predict, target_names=['Positive', 'Negative'])\n",
    "print(svm_rbf_bow_report)\n",
    "\n",
    "svm_rbf_tfidf_report = classification_report(test_sentiments, svm_rbf_tfidf_predict, target_names=['Positive', 'Negative'])\n",
    "print(svm_rbf_tfidf_report)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "151a10dc-bd05-489b-ad04-a58380747836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Confusion matrices\\ncm_bow_rbf = confusion_matrix(test_sentiments, svm_rbf_bow_predict, labels=[1, 0])\\nprint(cm_bow_rbf)\\n\\ncm_tfidf_rbf = confusion_matrix(test_sentiments, svm_rbf_tfidf_predict, labels=[1, 0])\\nprint(cm_tfidf_rbf)\\n'"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Confusion matrices\n",
    "cm_bow_rbf = confusion_matrix(test_sentiments, svm_rbf_bow_predict, labels=[1, 0])\n",
    "print(cm_bow_rbf)\n",
    "\n",
    "cm_tfidf_rbf = confusion_matrix(test_sentiments, svm_rbf_tfidf_predict, labels=[1, 0])\n",
    "print(cm_tfidf_rbf)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
